%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Constantin von Crailsheim at 2023-02-14 17:20:24 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{mohsenvand2020,
	abstract = {Interpreting and labeling human electroencephalogram (EEG) is a challenging task requiring years of medical training. We present a framework for learning representations from EEG signals via contrastive learning. By recombining channels from multi-channel recordings, we increase the number of samples quadratically per recording. We train a channel-wise feature extractor by extending the SimCLR framework to time-series data. We introduce a set of augmentations for EEG and study their efficacy on different classification tasks. We demonstrate that the learned features improve EEG classification and significantly reduce the amount of labeled data needed on three separate tasks: (1) Emotion Recognition (SEED), (2) Normal/Abnormal EEG classification (TUH), and (3) Sleep-stage scoring (SleepEDF). Our models show improved performance over previously reported supervised models on SEED and SleepEDF and self-supervised models on all three tasks.},
	author = {Mohsenvand, Mostafa Neo and Izadi, Mohammad Rasool and Maes, Pattie},
	booktitle = {Proceedings of the Machine Learning for Health NeurIPS Workshop},
	date-added = {2023-02-14 15:13:16 +0100},
	date-modified = {2023-02-14 15:13:31 +0100},
	editor = {Alsentzer, Emily and McDermott, Matthew B. A. and Falck, Fabian and Sarkar, Suproteem K. and Roy, Subhrajit and Hyland, Stephanie L.},
	month = {11 Dec},
	pages = {238--253},
	pdf = {http://proceedings.mlr.press/v136/mohsenvand20a/mohsenvand20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Contrastive Representation Learning for Electroencephalogram Classification},
	url = {https://proceedings.mlr.press/v136/mohsenvand20a.html},
	volume = {136},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v136/mohsenvand20a.html}}

@article{ching2020,
	abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems{\textemdash}patient classification, fundamental biological processes, and treatment of patients{\textemdash}and discuss whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H.S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
	date-added = {2023-02-14 12:09:53 +0100},
	date-modified = {2023-02-14 12:10:10 +0100},
	doi = {10.1101/142760},
	elocation-id = {142760},
	eprint = {https://www.biorxiv.org/content/early/2018/01/19/142760.full.pdf},
	journal = {bioRxiv},
	publisher = {Cold Spring Harbor Laboratory},
	title = {Opportunities and obstacles for deep learning in biology and medicine},
	url = {https://www.biorxiv.org/content/early/2018/01/19/142760},
	year = {2018},
	bdsk-url-1 = {https://www.biorxiv.org/content/early/2018/01/19/142760},
	bdsk-url-2 = {https://doi.org/10.1101/142760}}

@misc{ts2vec,
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2106.10466},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {T{S}2{V}ec: Towards Universal Representation of Time Series},
	url = {https://arxiv.org/abs/2106.10466},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.10466},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.10466}}

@misc{tstcc,
	author = {Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee-Keong and Li, Xiaoli and Guan, Cuntai},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2208.06616},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification},
	url = {https://arxiv.org/abs/2208.06616},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2208.06616},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2208.06616}}

@misc{oord2018,
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1807.03748},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Representation Learning with Contrastive Predictive Coding},
	url = {https://arxiv.org/abs/1807.03748},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1807.03748},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1807.03748}}

@article{simclr,
	author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2002-05709.bib},
	eprint = {2002.05709},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {https://arxiv.org/abs/2002.05709},
	volume = {abs/2002.05709},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2002.05709}}

@article{franceschi2019,
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1901.10738},
	keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Unsupervised Scalable Representation Learning for Multivariate Time Series},
	url = {https://arxiv.org/abs/1901.10738},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1901.10738},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1901.10738}}

@misc{tonekaboni2021,
	author = {Tonekaboni, Sana and Eytan, Danny and Goldenberg, Anna},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2106.00750},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding},
	url = {https://arxiv.org/abs/2106.00750},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.00750},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.00750}}

@misc{vaswani2017,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1706.03762},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Attention Is All You Need},
	url = {https://arxiv.org/abs/1706.03762},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1706.03762},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1706.03762}}

@article{bachman2019,
	author = {Philip Bachman and R. Devon Hjelm and William Buchwalter},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1906-00910.bib},
	date-modified = {2023-02-14 17:12:47 +0100},
	eprint = {1906.00910},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
	title = {Learning Representations by Maximizing Mutual Information Across Views},
	url = {http://arxiv.org/abs/1906.00910},
	volume = {abs/1906.00910},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1906.00910}}
